"""Predefined module catalog for common Kubernetes applications."""

MODULE_CATALOG = [
    # ── CI/CD & GitOps ──────────────────────────────────────────────────
    {
        "id": "arc-controller",
        "name": "Actions Runner Controller",
        "scope": "cluster",
        "description": "GitHub Actions Runner Controller (ARC) operator. Manages runner scale sets that auto-scale ephemeral self-hosted runners for GitHub Actions workflows. Install this once per cluster, then deploy runner scale sets per repository or organization.",
        "category": "ci-cd",
        "icon": "play",
        "repo_name": "",
        "repo_url": "",
        "chart_name": "oci://ghcr.io/actions/actions-runner-controller-charts/gha-runner-scale-set-controller",
        "default_namespace": "arc-systems",
        "default_release_name": "arc",
        "wizard_fields": [
            {
                "key": "replicaCount",
                "label": "Replicas",
                "description": "Number of controller pods. Increase for high availability.",
                "type": "number",
                "default": 1,
                "section": "Deployment",
            },
            {
                "key": "flags.logLevel",
                "label": "Log Level",
                "description": "Controller log verbosity.",
                "type": "select",
                "default": "info",
                "options": ["debug", "info", "warn", "error"],
                "section": "Logging",
            },
            {
                "key": "flags.updateStrategy",
                "label": "Update Strategy",
                "description": "How quickly runner scale sets react to changes. 'immediate' applies changes instantly, 'eventual' waits for current jobs to complete.",
                "type": "select",
                "default": "immediate",
                "options": ["immediate", "eventual"],
                "section": "Deployment",
            },
        ],
        "default_values": {},
        "privileged_namespace": True,
        "notes": "Install this controller first, then deploy one or more 'GitHub Actions Runner Set' instances. The controller watches all namespaces by default. Verify: kubectl get pods -n arc-systems",
    },
    {
        "id": "argocd",
        "name": "Argo CD",
        "scope": "cluster",
        "description": "Declarative GitOps continuous delivery for Kubernetes. Automatically syncs your cluster state to match Git repositories. Provides a web UI for visualizing deployments, diffs, and rollbacks.",
        "category": "ci-cd",
        "icon": "code-branch",
        "repo_name": "argo",
        "repo_url": "https://argoproj.github.io/argo-helm",
        "chart_name": "argo/argo-cd",
        "default_namespace": "argocd",
        "default_release_name": "argocd",
        "wizard_fields": [
            {
                "key": "server.service.type",
                "label": "Server Service Type",
                "description": "How to expose the Argo CD web UI. Use LoadBalancer with MetalLB for a dedicated IP.",
                "type": "select",
                "default": "ClusterIP",
                "options": ["ClusterIP", "LoadBalancer", "NodePort"],
                "section": "Server",
            },
            {
                "key": "configs.params.server\\.insecure",
                "label": "Disable TLS on Server",
                "description": "Run the Argo CD API server without TLS. Enable this if you're terminating TLS at an ingress controller.",
                "type": "boolean",
                "default": False,
                "section": "Server",
            },
            {
                "key": "server.replicas",
                "label": "Server Replicas",
                "description": "Number of Argo CD API server replicas.",
                "type": "number",
                "default": 1,
                "section": "Server",
            },
        ],
        "default_values": {},
        "notes": "Get the initial admin password with: kubectl -n argocd get secret argocd-initial-admin-secret -o jsonpath='{.data.password}' | base64 -d",
    },
    {
        "id": "arc-runner-set",
        "name": "GitHub Actions Runner Set",
        "scope": "application",
        "description": "Auto-scaling set of ephemeral GitHub Actions runners. Each install registers runners for a specific repository, organization, or enterprise. The release name becomes the 'runs-on' label in your workflows. Requires the ARC controller to be installed first.",
        "category": "ci-cd",
        "icon": "play",
        "repo_name": "",
        "repo_url": "",
        "chart_name": "oci://ghcr.io/actions/actions-runner-controller-charts/gha-runner-scale-set",
        "default_namespace": "",
        "default_release_name": "",
        "wizard_fields": [
            {
                "key": "githubConfigUrl",
                "label": "GitHub Config URL",
                "description": "Repository, organization, or enterprise URL to register runners against. Examples: https://github.com/myorg/myrepo or https://github.com/myorg",
                "type": "text",
                "default": "",
                "placeholder": "https://github.com/org/repo",
                "section": "GitHub",
            },
            {
                "key": "githubConfigSecret.github_token",
                "label": "GitHub PAT (Classic)",
                "description": "Classic Personal Access Token (ghp_...). Org-level: requires admin:org scope. Repo-level: requires repo scope. Fine-grained PATs (github_pat_) are NOT supported for org-level runners.",
                "type": "text",
                "default": "",
                "placeholder": "ghp_...",
                "section": "GitHub",
            },
            {
                "key": "maxRunners",
                "label": "Max Runners",
                "description": "Maximum number of runner pods. Limits concurrent workflow jobs.",
                "type": "number",
                "default": 10,
                "section": "Scaling",
            },
            {
                "key": "minRunners",
                "label": "Min Runners",
                "description": "Minimum idle runner pods. Set >0 to keep warm runners for faster job starts (uses more resources).",
                "type": "number",
                "default": 0,
                "section": "Scaling",
            },
            {
                "key": "containerMode.type",
                "label": "Container Mode",
                "description": "How to run container actions/services. 'dind' uses Docker-in-Docker (requires privileged). 'kubernetes' creates job containers as pods. Leave empty for no container support.",
                "type": "select",
                "default": "dind",
                "options": ["dind", "kubernetes"],
                "section": "Runner",
            },
            {
                "key": "runnerGroup",
                "label": "Runner Group",
                "description": "GitHub runner group name. Leave empty for the organization default. Custom groups require GitHub Team or Enterprise plan.",
                "type": "text",
                "default": "",
                "section": "Runner",
            },
            {
                "key": "containerMode.kubernetesModeWorkVolumeClaim.storageClassName",
                "label": "Work Volume Storage Class",
                "description": "StorageClass for ephemeral work volumes. Must support ReadWriteOnce access.",
                "type": "text",
                "default": "longhorn",
                "section": "Runner",
                "show_when": {"key": "containerMode.type", "value": "kubernetes"},
            },
            {
                "key": "containerMode.kubernetesModeWorkVolumeClaim.resources.requests.storage",
                "label": "Work Volume Size",
                "description": "Size of ephemeral work volume for each runner pod.",
                "type": "text",
                "default": "1Gi",
                "section": "Runner",
                "show_when": {"key": "containerMode.type", "value": "kubernetes"},
            },
        ],
        "default_values": {
            "containerMode": {
                "kubernetesModeWorkVolumeClaim": {
                    "accessModes": ["ReadWriteOnce"],
                },
            },
        },
        "privileged_namespace": True,
        "notes": "The release name (e.g., 'arc-runner-set') becomes the 'runs-on:' label in GitHub Actions workflows. Example: runs-on: arc-runner-set. Verify runners: kubectl get pods -n <namespace>. Requires ARC controller installed in arc-systems namespace.",
    },
    {
        "id": "harbor",
        "name": "Harbor",
        "scope": "cluster",
        "description": "Enterprise-grade OCI artifact registry with vulnerability scanning, RBAC, and replication. Stores container images, Helm charts, and OCI artifacts. Includes Trivy scanner for automated security analysis.",
        "category": "ci-cd",
        "icon": "cube",
        "repo_name": "harbor",
        "repo_url": "https://helm.goharbor.io",
        "chart_name": "harbor/harbor",
        "default_namespace": "harbor",
        "default_release_name": "harbor",
        "wizard_fields": [
            {
                "key": "expose.type",
                "label": "Expose Method",
                "description": "How to expose Harbor externally. Use LoadBalancer with MetalLB for a dedicated IP, NodePort for direct node access, or Ingress if you have an ingress controller like Traefik.",
                "type": "select",
                "default": "nodePort",
                "options": ["ingress", "nodePort", "loadBalancer", "clusterIP"],
                "section": "Networking",
            },
            {
                "key": "expose.tls.enabled",
                "label": "Enable TLS",
                "description": "Enable HTTPS for Harbor. When enabled with certSource=auto, Harbor generates self-signed certificates.",
                "type": "boolean",
                "default": True,
                "section": "Networking",
            },
            {
                "key": "externalURL",
                "label": "External URL",
                "description": "The URL used to access Harbor from outside the cluster. Must include protocol and match your expose method. Example: https://harbor.example.com or https://<node-ip>:30003",
                "type": "text",
                "default": "",
                "placeholder": "https://harbor.example.com",
                "section": "Networking",
            },
            {
                "key": "harborAdminPassword",
                "label": "Admin Password",
                "description": "Password for the Harbor 'admin' user. Change this from the default for security.",
                "type": "text",
                "default": "Harbor12345",
                "required": True,
                "section": "Authentication",
            },
            {
                "key": "persistence.enabled",
                "label": "Enable Persistence",
                "description": "Use persistent volumes for registry data, database, and Redis. Requires a StorageClass (e.g., Longhorn).",
                "type": "boolean",
                "default": True,
                "section": "Storage",
            },
            {
                "key": "persistence.persistentVolumeClaim.registry.size",
                "label": "Registry Volume Size",
                "description": "Storage size for container images and artifacts. Size depends on how many images you plan to store.",
                "type": "text",
                "default": "50Gi",
                "section": "Storage",
            },
            {
                "key": "persistence.persistentVolumeClaim.database.size",
                "label": "Database Volume Size",
                "description": "Storage size for the internal PostgreSQL database.",
                "type": "text",
                "default": "5Gi",
                "section": "Storage",
            },
            {
                "key": "trivy.enabled",
                "label": "Enable Trivy Scanner",
                "description": "Deploy Trivy for automatic vulnerability scanning of pushed images. Recommended for security auditing.",
                "type": "boolean",
                "default": True,
                "section": "Security",
            },
            {
                "key": "metrics.enabled",
                "label": "Enable Metrics",
                "description": "Expose Prometheus metrics for Harbor components. Works with kube-prometheus-stack.",
                "type": "boolean",
                "default": False,
                "section": "Monitoring",
            },
        ],
        "default_values": {},
        "notes": "Access Harbor at the configured external URL. Default credentials: admin / (your configured password). Push images: docker login <harbor-url>, docker tag <image> <harbor-url>/<project>/<image>:<tag>, docker push. For NodePort, the HTTPS port defaults to 30003.",
    },

    # ── Databases ────────────────────────────────────────────────────────
    {
        "id": "mariadb",
        "name": "MariaDB",
        "scope": "application",
        "description": "MySQL-compatible relational database with optional Galera multi-master clustering. Security-hardened with non-root containers and read-only root filesystem.",
        "category": "databases",
        "icon": "database",
        "repo_name": "",
        "repo_url": "",
        "chart_name": "oci://registry-1.docker.io/cloudpirates/mariadb",
        "default_namespace": "",
        "default_release_name": "",
        "wizard_fields": [
            {
                "key": "auth.rootPassword",
                "label": "Root Password",
                "description": "Password for the root user. Leave empty to auto-generate a random password stored in a Kubernetes Secret.",
                "type": "text",
                "default": "",
                "placeholder": "Leave empty to auto-generate",
                "section": "Authentication",
            },
            {
                "key": "auth.database",
                "label": "Default Database",
                "description": "Name of the default database to create on startup.",
                "type": "text",
                "default": "my_database",
                "section": "Authentication",
            },
            {
                "key": "auth.username",
                "label": "Username",
                "description": "Custom database user (in addition to root). Leave empty to skip.",
                "type": "text",
                "default": "",
                "placeholder": "Optional",
                "section": "Authentication",
            },
            {
                "key": "auth.password",
                "label": "User Password",
                "description": "Password for the custom user. Leave empty to auto-generate.",
                "type": "text",
                "default": "",
                "placeholder": "Leave empty to auto-generate",
                "section": "Authentication",
            },
            {
                "key": "persistence.enabled",
                "label": "Enable Persistence",
                "description": "Attach a persistent volume for database storage. Requires a StorageClass (e.g., Longhorn).",
                "type": "boolean",
                "default": True,
                "section": "Storage",
            },
            {
                "key": "persistence.size",
                "label": "Volume Size",
                "description": "Size of the persistent volume for MariaDB data.",
                "type": "text",
                "default": "8Gi",
                "section": "Storage",
            },
            {
                "key": "galera.enabled",
                "label": "Enable Galera Cluster",
                "description": "Deploy a multi-master Galera cluster instead of a single standalone instance. All nodes can accept reads and writes.",
                "type": "boolean",
                "default": False,
                "section": "Clustering",
            },
            {
                "key": "galera.replicaCount",
                "label": "Cluster Nodes",
                "description": "Number of nodes in the Galera cluster. Minimum 3 recommended for quorum. All nodes are equal peers (multi-master).",
                "type": "number",
                "default": 3,
                "section": "Clustering",
                "show_when": {"key": "galera.enabled", "value": True},
            },
            {
                "key": "resources.requests.memory",
                "label": "Memory Request",
                "description": "Minimum memory for each MariaDB pod.",
                "type": "text",
                "default": "256Mi",
                "section": "Resources",
            },
            {
                "key": "resources.requests.cpu",
                "label": "CPU Request",
                "description": "Minimum CPU for each MariaDB pod.",
                "type": "text",
                "default": "100m",
                "section": "Resources",
            },
            {
                "key": "metrics.enabled",
                "label": "Enable Metrics",
                "description": "Deploy a Prometheus mysqld_exporter sidecar for MariaDB metrics. Works with kube-prometheus-stack.",
                "type": "boolean",
                "default": False,
                "section": "Monitoring",
            },
        ],
        "default_values": {},
        "notes": "Connect: <release>-mariadb.<namespace>.svc.cluster.local:3306. Get root password: kubectl get secret <release>-mariadb -n <namespace> -o jsonpath='{.data.mariadb-root-password}' | base64 -d. Galera cluster: all nodes accept writes, connect to the headless service for cluster-aware clients.",
    },
    {
        "id": "postgresql",
        "name": "PostgreSQL",
        "scope": "application",
        "description": "Reliable relational database with authentication, persistence, and Prometheus metrics. Security-hardened with non-root containers. Single-instance deployment suited for application backends.",
        "category": "databases",
        "icon": "database",
        "repo_name": "",
        "repo_url": "",
        "chart_name": "oci://registry-1.docker.io/cloudpirates/postgres",
        "default_namespace": "",
        "default_release_name": "",
        "wizard_fields": [
            {
                "key": "auth.username",
                "label": "Username",
                "description": "PostgreSQL superuser name.",
                "type": "text",
                "default": "postgres",
                "section": "Authentication",
            },
            {
                "key": "auth.password",
                "label": "Password",
                "description": "Superuser password. Leave empty to auto-generate a random password stored in a Kubernetes Secret.",
                "type": "text",
                "default": "",
                "placeholder": "Leave empty to auto-generate",
                "section": "Authentication",
            },
            {
                "key": "auth.database",
                "label": "Default Database",
                "description": "Name of the default database to create on startup.",
                "type": "text",
                "default": "",
                "placeholder": "postgres (default)",
                "section": "Authentication",
            },
            {
                "key": "persistence.enabled",
                "label": "Enable Persistence",
                "description": "Attach a persistent volume for database storage. Requires a StorageClass (e.g., Longhorn).",
                "type": "boolean",
                "default": True,
                "section": "Storage",
            },
            {
                "key": "persistence.size",
                "label": "Volume Size",
                "description": "Size of the persistent volume for PostgreSQL data.",
                "type": "text",
                "default": "8Gi",
                "section": "Storage",
            },
            {
                "key": "resources.requests.memory",
                "label": "Memory Request",
                "description": "Minimum memory for the PostgreSQL pod.",
                "type": "text",
                "default": "256Mi",
                "section": "Resources",
            },
            {
                "key": "resources.requests.cpu",
                "label": "CPU Request",
                "description": "Minimum CPU for the PostgreSQL pod.",
                "type": "text",
                "default": "100m",
                "section": "Resources",
            },
            {
                "key": "metrics.enabled",
                "label": "Enable Metrics",
                "description": "Deploy a Prometheus exporter sidecar for PostgreSQL metrics. Works with kube-prometheus-stack.",
                "type": "boolean",
                "default": False,
                "section": "Monitoring",
            },
        ],
        "default_values": {},
        "notes": "Connect from within the cluster: <release>-postgres.<namespace>.svc.cluster.local:5432. Get the auto-generated password: kubectl get secret <release>-postgres -n <namespace> -o jsonpath='{.data.postgres-password}' | base64 -d",
    },
    {
        "id": "redis",
        "name": "Redis",
        "scope": "application",
        "description": "In-memory data store supporting standalone, master-replica with Sentinel HA, and multi-master cluster architectures. Security-hardened with non-root containers and read-only filesystems.",
        "category": "databases",
        "icon": "database",
        "repo_name": "",
        "repo_url": "",
        "chart_name": "oci://registry-1.docker.io/cloudpirates/redis",
        "default_namespace": "",
        "default_release_name": "",
        "wizard_fields": [
            {
                "key": "architecture",
                "label": "Architecture",
                "description": "standalone: single instance. replication: master + read replicas with optional Sentinel failover. cluster: multi-master sharded cluster with automatic slot distribution.",
                "type": "select",
                "default": "replication",
                "options": ["standalone", "replication", "cluster"],
                "section": "Topology",
            },
            {
                "key": "replicaCount",
                "label": "Node Count",
                "description": "Total Redis nodes. Replication: 1 master + (N-1) read replicas. Cluster: total nodes must be a multiple of (clusterReplicaCount + 1), minimum 3.",
                "type": "number",
                "default": 3,
                "max": 12,
                "section": "Topology",
            },
            {
                "key": "clusterReplicaCount",
                "label": "Replicas per Master",
                "description": "Number of replica nodes per master shard. Example: 6 nodes with 1 replica = 3 masters + 3 replicas. Set to 0 for masters only.",
                "type": "number",
                "default": 0,
                "section": "Cluster",
                "show_when": {"key": "architecture", "value": "cluster"},
            },
            {
                "key": "cluster.config.nodeTimeout",
                "label": "Node Timeout (ms)",
                "description": "Time in milliseconds a node must be unreachable before it's considered failed.",
                "type": "number",
                "default": 15000,
                "section": "Cluster",
                "show_when": {"key": "architecture", "value": "cluster"},
            },
            {
                "key": "cluster.config.requireFullCoverage",
                "label": "Require Full Coverage",
                "description": "When enabled, the cluster stops accepting writes if any hash slot is not covered. Disable to allow partial availability.",
                "type": "boolean",
                "default": True,
                "section": "Cluster",
                "show_when": {"key": "architecture", "value": "cluster"},
            },
            {
                "key": "sentinel.enabled",
                "label": "Enable Sentinel",
                "description": "Deploy Sentinel sidecars for automatic master failover.",
                "type": "boolean",
                "default": False,
                "section": "High Availability",
                "show_when": {"key": "architecture", "value": "replication"},
            },
            {
                "key": "sentinel.quorum",
                "label": "Sentinel Quorum",
                "description": "Number of Sentinels that must agree the master is down before triggering failover.",
                "type": "number",
                "default": 2,
                "section": "High Availability",
                "show_when": {"key": "architecture", "value": "replication"},
            },
            {
                "key": "auth.enabled",
                "label": "Enable Authentication",
                "description": "Require a password to connect. When enabled with no password set, a random one is auto-generated in a Kubernetes Secret.",
                "type": "boolean",
                "default": True,
                "section": "Security",
            },
            {
                "key": "auth.password",
                "label": "Redis Password",
                "description": "Password for Redis connections. Leave empty to auto-generate a random password stored in a Kubernetes Secret.",
                "type": "text",
                "default": "",
                "placeholder": "Leave empty to auto-generate",
                "section": "Security",
            },
            {
                "key": "persistence.enabled",
                "label": "Enable Persistence",
                "description": "Attach a persistent volume to each Redis pod. Requires a StorageClass (e.g., Longhorn).",
                "type": "boolean",
                "default": True,
                "section": "Storage",
            },
            {
                "key": "persistence.size",
                "label": "Volume Size",
                "description": "Size of the persistent volume for each Redis pod.",
                "type": "text",
                "default": "8Gi",
                "section": "Storage",
            },
            {
                "key": "resources.requests.memory",
                "label": "Memory Request",
                "description": "Minimum memory allocated to each Redis pod.",
                "type": "text",
                "default": "128Mi",
                "section": "Resources",
            },
            {
                "key": "resources.limits.memory",
                "label": "Memory Limit",
                "description": "Maximum memory each Redis pod can use.",
                "type": "text",
                "default": "256Mi",
                "section": "Resources",
            },
            {
                "key": "metrics.enabled",
                "label": "Enable Metrics Exporter",
                "description": "Deploy a Prometheus Redis exporter sidecar. Works with kube-prometheus-stack.",
                "type": "boolean",
                "default": False,
                "section": "Monitoring",
            },
            {
                "key": "pdb.enabled",
                "label": "Pod Disruption Budget",
                "description": "Create a PodDisruptionBudget to limit voluntary disruptions during upgrades or node drains.",
                "type": "boolean",
                "default": False,
                "section": "Availability",
            },
        ],
        "default_values": {},
        "notes": "Standalone/Replication: connect to <release>-redis-master.<namespace>:6379. Cluster: connect to any node on port 6379, clients must support MOVED/ASK redirects (e.g., redis-cli -c). Get password: kubectl get secret <release>-redis -n <namespace> -o jsonpath='{.data.redis-password}' | base64 -d. Storage: each node creates a PVC. With Longhorn (default 3 replicas), total cluster storage is approximately nodeCount x volumeSize x 3. For a 6-node cluster with 8Gi volumes: 6 x 8 x 3 = 144Gi.",
    },

    # ── Monitoring ───────────────────────────────────────────────────────
    {
        "id": "metrics-server",
        "name": "Metrics Server",
        "scope": "cluster",
        "description": "Lightweight cluster-wide resource metrics aggregator. Required for kubectl top, Horizontal Pod Autoscaler (HPA), and Vertical Pod Autoscaler (VPA). Collects CPU and memory usage from kubelets.",
        "category": "monitoring",
        "icon": "chart-bar",
        "repo_name": "metrics-server",
        "repo_url": "https://kubernetes-sigs.github.io/metrics-server/",
        "chart_name": "metrics-server/metrics-server",
        "default_namespace": "kube-system",
        "default_release_name": "metrics-server",
        "wizard_fields": [
            {
                "key": "_kubeletInsecureTls",
                "label": "Skip Kubelet TLS Verification",
                "description": "Required for Talos Linux and other distributions that use self-signed kubelet certificates. Adds --kubelet-insecure-tls to the metrics-server args.",
                "type": "boolean",
                "default": True,
                "section": "TLS",
            },
            {
                "key": "replicas",
                "label": "Replicas",
                "description": "Number of metrics-server pods. Use 2+ for high availability.",
                "type": "number",
                "default": 1,
                "section": "Deployment",
            },
            {
                "key": "_metricResolution",
                "label": "Metric Resolution",
                "description": "How often metrics are scraped from kubelets. Lower values increase load but give fresher data.",
                "type": "select",
                "default": "15s",
                "options": ["10s", "15s", "30s", "60s"],
                "section": "Collection",
            },
        ],
        "default_values": {},
        "notes": "After installation, verify with: kubectl top nodes. If metrics are unavailable, ensure --kubelet-insecure-tls is enabled (required for Talos Linux). HPA and VPA will start working automatically once metrics are available.",
    },
    {
        "id": "kube-prometheus-stack",
        "name": "Prometheus + Grafana",
        "scope": "cluster",
        "description": "Complete cluster monitoring stack. Includes Prometheus for metrics collection, Grafana for dashboards, Alertmanager for alerts, and node-exporter for host metrics. Comes with pre-configured dashboards for Kubernetes.",
        "category": "monitoring",
        "icon": "chart-bar",
        "repo_name": "prometheus-community",
        "repo_url": "https://prometheus-community.github.io/helm-charts",
        "chart_name": "prometheus-community/kube-prometheus-stack",
        "default_namespace": "monitoring",
        "default_release_name": "kube-prometheus-stack",
        "wizard_fields": [
            {
                "key": "grafana.adminPassword",
                "label": "Grafana Admin Password",
                "description": "Password for the Grafana 'admin' user. Change this from the default for security.",
                "type": "text",
                "default": "admin",
                "required": True,
                "section": "Grafana",
            },
            {
                "key": "grafana.service.type",
                "label": "Grafana Service Type",
                "description": "How to expose Grafana. Use LoadBalancer with MetalLB for a dedicated IP, or NodePort for direct access.",
                "type": "select",
                "default": "ClusterIP",
                "options": ["ClusterIP", "LoadBalancer", "NodePort"],
                "section": "Grafana",
            },
            {
                "key": "prometheus.prometheusSpec.retention",
                "label": "Metrics Retention",
                "description": "How long to keep metrics data. Longer retention uses more storage.",
                "type": "text",
                "default": "15d",
                "placeholder": "e.g., 15d, 30d, 90d",
                "section": "Prometheus",
            },
            {
                "key": "prometheus.prometheusSpec.resources.requests.memory",
                "label": "Prometheus Memory Request",
                "description": "Minimum memory for Prometheus. Increase for large clusters with many metrics.",
                "type": "text",
                "default": "512Mi",
                "section": "Prometheus",
            },
        ],
        "default_values": {},
        "notes": "Access Grafana at its service IP/port. Default credentials: admin / (your configured password). Pre-configured dashboards are available under 'Dashboards' in the left sidebar.",
    },

    # ── Networking ───────────────────────────────────────────────────────
    {
        "id": "metallb",
        "name": "MetalLB",
        "scope": "cluster",
        "description": "Bare-metal load balancer for Kubernetes. Assigns real IP addresses to LoadBalancer-type Services without needing a cloud provider. Uses Layer 2 (ARP) or BGP to advertise IPs on your network.",
        "category": "networking",
        "icon": "globe",
        "repo_name": "metallb",
        "repo_url": "https://metallb.github.io/metallb",
        "chart_name": "metallb/metallb",
        "default_namespace": "metallb-system",
        "default_release_name": "metallb",
        "wizard_fields": [
            {
                "key": "_addressPool",
                "label": "IP Address Pool",
                "description": "Range of IPs MetalLB can assign to LoadBalancer services. Must be routable on your network and not overlap with DHCP. Example: 10.0.200.1-10.0.200.50 or 10.0.200.0/24",
                "type": "text",
                "default": "",
                "placeholder": "e.g., 10.0.200.1-10.0.200.50",
                "required": True,
                "section": "Network",
            },
        ],
        "default_values": {},
        "privileged_namespace": True,
        "notes": "After installation, MetalLB needs an IPAddressPool and L2Advertisement custom resource to start assigning IPs. Ktizo will create these automatically based on your configured address pool.",
    },
    {
        "id": "traefik",
        "name": "Traefik",
        "scope": "cluster",
        "description": "Modern reverse proxy and ingress controller. Automatically discovers services and routes traffic to them. Supports HTTP/HTTPS, TCP, UDP, gRPC, and WebSocket with automatic TLS via Let's Encrypt.",
        "category": "networking",
        "icon": "globe",
        "repo_name": "traefik",
        "repo_url": "https://traefik.github.io/charts",
        "chart_name": "traefik/traefik",
        "default_namespace": "traefik",
        "default_release_name": "traefik",
        "wizard_fields": [
            {
                "key": "service.type",
                "label": "Service Type",
                "description": "How to expose Traefik. Use LoadBalancer if you have MetalLB, NodePort to access via node IPs.",
                "type": "select",
                "default": "LoadBalancer",
                "options": ["LoadBalancer", "NodePort", "ClusterIP"],
                "section": "Networking",
            },
            {
                "key": "ports.websecure.http.tls.enabled",
                "label": "Enable TLS on HTTPS Port",
                "description": "Enable TLS termination on the websecure entrypoint (port 443). Chart path: ports.websecure.http.tls.enabled.",
                "type": "boolean",
                "default": True,
                "section": "TLS",
            },
            {
                "key": "ingressRoute.dashboard.enabled",
                "label": "Enable Dashboard",
                "description": "Deploy the Traefik dashboard IngressRoute for monitoring and debugging.",
                "type": "boolean",
                "default": False,
                "section": "Dashboard",
            },
            {
                "key": "deployment.replicas",
                "label": "Replicas",
                "description": "Number of Traefik pods. Increase for high availability.",
                "type": "number",
                "default": 1,
                "section": "Deployment",
            },
            {
                "key": "logs.general.level",
                "label": "Log Level",
                "description": "Traefik log verbosity level.",
                "type": "select",
                "default": "INFO",
                "options": ["TRACE", "DEBUG", "INFO", "WARN", "ERROR"],
                "section": "Logging",
            },
        ],
        "default_values": {},
        "notes": "Traefik will create a LoadBalancer service. Use IngressRoute CRDs or standard Kubernetes Ingress resources to route traffic.",
    },

    # ── Security ─────────────────────────────────────────────────────────
    {
        "id": "cert-manager",
        "name": "cert-manager",
        "scope": "cluster",
        "description": "Automated TLS certificate management for Kubernetes. Integrates with Let's Encrypt, HashiCorp Vault, Venafi, and self-signed CAs. Automatically issues and renews certificates for your Ingress resources.",
        "category": "security",
        "icon": "lock",
        "repo_name": "jetstack",
        "repo_url": "https://charts.jetstack.io",
        "chart_name": "jetstack/cert-manager",
        "default_namespace": "cert-manager",
        "default_release_name": "cert-manager",
        "wizard_fields": [
            {
                "key": "crds.enabled",
                "label": "Install CRDs",
                "description": "Install cert-manager Custom Resource Definitions. Required for first-time installation.",
                "type": "boolean",
                "default": True,
                "section": "General",
            },
            {
                "key": "replicaCount",
                "label": "Replica Count",
                "description": "Number of cert-manager controller pods.",
                "type": "number",
                "default": 1,
                "section": "General",
            },
        ],
        "default_values": {"crds": {"enabled": True}},
        "notes": "After installation, create a ClusterIssuer resource for Let's Encrypt or your CA. Then annotate Ingress resources with cert-manager.io/cluster-issuer to get automatic TLS certificates.",
    },

    # ── Storage ──────────────────────────────────────────────────────────
    {
        "id": "longhorn",
        "name": "Longhorn",
        "scope": "cluster",
        "description": "Cloud-native distributed storage for Kubernetes. Provides replicated persistent volumes with built-in backup, restore, and snapshot support. Ideal for clusters that need simple, reliable storage without external dependencies.",
        "category": "storage",
        "icon": "hard-drive",
        "repo_name": "longhorn",
        "repo_url": "https://charts.longhorn.io",
        "chart_name": "longhorn/longhorn",
        "default_namespace": "longhorn-system",
        "default_release_name": "longhorn",
        "wizard_fields": [
            {
                "key": "_partition.enabled",
                "label": "Enable Storage Provisioning",
                "description": "Provision Longhorn storage on each node. If EPHEMERAL is capped in Storage Settings, a separate partition is created. If EPHEMERAL fills the disk, Longhorn uses /var/lib/longhorn inside it (no extra partition needed).",
                "type": "boolean",
                "default": True,
                "section": "Node Storage",
            },
            {
                "key": "_partition.mountpoint",
                "label": "Partition Mount Path",
                "description": "Mount path for the dedicated partition (only used when EPHEMERAL is capped).",
                "type": "text",
                "default": "/var/mnt/longhorn",
                "section": "Node Storage",
            },
            {
                "key": "defaultSettings.defaultDataPath",
                "label": "Default Data Path",
                "description": "Path where Longhorn stores volume data. Auto-adjusted based on EPHEMERAL config: /var/mnt/longhorn (capped) or /var/lib/longhorn (full disk).",
                "type": "text",
                "default": "/var/mnt/longhorn",
                "section": "Node Storage",
            },
            {
                "key": "persistence.defaultClass",
                "label": "Default StorageClass",
                "description": "Set Longhorn as the default Kubernetes StorageClass. New PVCs without a storageClassName will use Longhorn.",
                "type": "boolean",
                "default": True,
                "section": "Kubernetes",
            },
            {
                "key": "defaultSettings.defaultReplicaCount",
                "label": "Default Replica Count",
                "description": "Number of copies of each volume across nodes. Use 3 for production HA, 1 for single-node clusters.",
                "type": "number",
                "default": 3,
                "section": "Storage",
            },
            {
                "key": "defaultSettings.defaultDataLocality",
                "label": "Data Locality",
                "description": "Controls whether data is kept on the same node as the workload. 'best-effort' improves read performance, 'disabled' spreads data evenly.",
                "type": "select",
                "default": "disabled",
                "options": ["disabled", "best-effort", "strict-local"],
                "section": "Storage",
            },
            {
                "key": "persistence.defaultClassReplicaCount",
                "label": "StorageClass Replica Count",
                "description": "Replica count for the default Longhorn StorageClass. Should match or be less than Default Replica Count.",
                "type": "number",
                "default": 3,
                "section": "Storage",
            },
            {
                "key": "longhornUI.replicas",
                "label": "UI Replicas",
                "description": "Number of Longhorn UI dashboard pod replicas.",
                "type": "number",
                "default": 1,
                "section": "UI",
            },
        ],
        "default_values": {},
        "privileged_namespace": True,
        "notes": "After installation, Longhorn provides a web UI accessible via its service. Create a LoadBalancer or Ingress to access it.",
    },
    {
        "id": "rook-ceph",
        "name": "Rook Ceph",
        "scope": "cluster",
        "description": "Production-grade distributed storage orchestrator for Ceph. Provides block, object, and shared filesystem storage. More complex than Longhorn but scales to thousands of nodes. Requires dedicated disks on at least 3 nodes.",
        "category": "storage",
        "icon": "hard-drive",
        "repo_name": "rook-release",
        "repo_url": "https://charts.rook.io/release",
        "chart_name": "rook-release/rook-ceph",
        "default_namespace": "rook-ceph",
        "default_release_name": "rook-ceph",
        "wizard_fields": [
            {
                "key": "crds.enabled",
                "label": "Install CRDs",
                "description": "Install Rook Custom Resource Definitions. Required for first-time installation.",
                "type": "boolean",
                "default": True,
                "section": "General",
            },
            {
                "key": "monitoring.enabled",
                "label": "Enable Monitoring",
                "description": "Enable Prometheus ServiceMonitor for Rook operator metrics. Requires kube-prometheus-stack.",
                "type": "boolean",
                "default": False,
                "section": "Monitoring",
            },
        ],
        "default_values": {},
        "notes": "This installs the Rook operator only. You still need to create a CephCluster CR to provision storage. See the Rook documentation for cluster configuration.",
    },
]


def get_catalog() -> list:
    """Return the full module catalog."""
    return MODULE_CATALOG


def get_catalog_entry(catalog_id: str) -> dict | None:
    """Look up a single catalog entry by ID."""
    for entry in MODULE_CATALOG:
        if entry["id"] == catalog_id:
            return entry
    return None
